

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>第二章 一个可靠的存储后端 &mdash; In the Cloud  文档</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="In the Cloud  文档" href="../index.html"/>
        <link rel="next" title="第三章 合适的虚拟化平台" href="ch03.html"/>
        <link rel="prev" title="第一章 随便说些什么" href="ch01.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> In the Cloud
          

          
          </a>

          
            
            
              <div class="version">
                0.8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="ch01.html">第一章 随便说些什么</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="">第二章 一个可靠的存储后端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">2.1 谈谈分布式存储</a></li>
<li class="toctree-l2"><a class="reference internal" href="#glusterfs">2.2 Glusterfs简述</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">功能介绍</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">组合方式</a></li>
<li class="toctree-l3"><a class="reference internal" href="#translator">Translator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#afr">AFR</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dht">DHT</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id5">2.3 搭建Glusterfs作为基础存储</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dnshosts">添加DNS或者修改hosts文件</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">准备磁盘作为砖块</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">添加卷</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">挂载卷</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id9">2.4 Glusterfs应用示例及技巧</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id10">参数调整</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">文件权限</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">砖块组合</a></li>
<li class="toctree-l3"><a class="reference internal" href="#normalreplicastriped">normal、replica、striped卷组合</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nfs">作为nfs挂载</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cifs">作为cifs挂载</a></li>
<li class="toctree-l3"><a class="reference internal" href="#split-brain">修复裂脑（split-brain）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id13">砖块复用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ip">高可用业务IP</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ch03.html">第三章 合适的虚拟化平台</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch04.html">第四章 数据抓取与机器学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch05.html">第五章 数据处理平台</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix01.html">附录一 OpenStack概念、部署、与高级网络应用</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix02.html">附录二 公有云参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix03.html">附录三 PaaS/OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix04.html">附录四  Docker 使用及自建repo</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix05.html">附录五 常用功能运维工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix06.html">附录六 文档参考资源以及建议书单</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix07.html">待整理扩展内容</a></li>
<li class="toctree-l1"><a class="reference internal" href="exp.html">实践 构建先进的家居云</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="about.html">关于作者与文档</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">In the Cloud</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
    <li>第二章 一个可靠的存储后端</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/posts/ch02.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>第二章 一个可靠的存储后端<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<div class="section" id="id2">
<h2>2.1 谈谈分布式存储<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>计算机领域中有诸多有意思的东西可以把玩，在这儿且看看分布式存储。</p>
<p><strong>集群文件系统</strong></p>
<p>在某些场景下又可以称作网络文件系统、并行文件系统，在70年代由IBM提出并实现原型。</p>
<p>有几种方法可以实现集群形式，但多数仅仅是节点直连存储而不是将存储之上的文件系统进行合理“分布”。分布式文件系统同时挂载于多个服务器上，并在它们之间共享，可以提供类似于位置无关的数据定位或冗余等特点。并行文件系统是一种集群式的文件系统，它将数据分布于多个节点，其主要目的是提供冗余和提高读写性能。</p>
<p><strong>共享磁盘（Shared-disk）/Storage-area network(SAN)</strong></p>
<p>从应用程序使用的文件级别，到SAN之间的块级别的操作，诸如权限控制和传输，都是发生在客户端节点上。共享磁盘（Shared-disk）文件系统，在并行控制上做了很多工作，以至于其拥有比较一致连贯的文件系统视图，从而避免了多个客户端试图同时访问同一设备时数据断片或丢失的情况发生。其中有种技术叫做围栏（Fencing），就是在某个或某些节点发生断片时，集群自动将这些节点隔离（关机、断网、自恢复），保证其他节点数据访问的正确性。元数据（Metadata）类似目录，可以让所有的机器都能查找使用所有信息，在不同的架构中有不同的保存方式，有的均匀分布于集群，有的存储在中央节点。</p>
<p>实现的方式有iSCSI，AoE，FC，Infiniband等，比较著名的产品有Redhat GFS、Sun QFS、Vmware VMFS等。</p>
<p><strong>分布式文件系统</strong></p>
<p>分布式文件系统则不是块级别的共享的形式了，所有加进来的存储（文件系统）都是整个文件系统的一部分，所有数据的传输也是依靠网络来的。</p>
<p>它的设计有这么几个原则：</p>
<ul class="simple">
<li><em>访问透明</em> 客户端在其上的文件操作与本地文件系统无异</li>
<li><em>位置透明</em> 其上的文件不代表其存储位置，只要给了全名就能访问</li>
<li><em>并发透明</em> 所有客户端持有的文件系统的状态在任何时候都是一致的，不会出现A修改了F文件，但是B愣了半天才发现。</li>
<li><em>失败透明</em> 理解为阻塞操作，不成功不回头。</li>
<li><em>异构性</em> 文件系统可以在多种硬件以及操作系统下部署使用。</li>
<li><em>扩展性</em> 随时添加进新的节点，无视其资格新旧。</li>
<li><em>冗余透明</em> 客户端不需要了解文件存在于多个节点上这一事实。</li>
<li><em>迁移透明</em> 客户端不需要了解文件根据负载均衡策略迁移的状况。</li>
</ul>
<p>实现的方式有NFS、CIFS、SMB、NCP等，比较著名的产品有Google GFS、Hadoop HDFS、GlusterFS、Lustre等。</p>
<blockquote class="epigraph">
<div><p>FUSE，filesystem in user space。</p>
<p>FUSE全名Filesystem in Userspace，是在类UNIX系统下的一个机制，可以让普通用户创建修改访问文件系统。功能就是连接内核接口与用户控件程序的一座“桥”，目前普遍存在于多个操作系统中，比如Linux、BSD、Solaris、OSX、Android等。</p>
<p>FUSE来源于AVFS，不同于传统文件系统从磁盘读写数据，FUSE在文件系统或磁盘中有“转换”的角色，本身并不会存储数据。</p>
<p>在Linux系统中的实现有很多，比如各种要挂载ntfs文件系统使用到的ntfs-3g，以及即将要用到的glusterfs-fuse。</p>
</div></blockquote>
<img alt="../_images/02-01.png" class="align-center" src="../_images/02-01.png" />
</div>
<div class="section" id="glusterfs">
<h2>2.2 Glusterfs简述<a class="headerlink" href="#glusterfs" title="永久链接至标题">¶</a></h2>
<p>接下来，说一下我所看到的glusterfs。</p>
<p>首先它可以基于以太网或者Infiniband构建大规模分布式文件系统，其设计原则符合奥卡姆剃刀原则，即“ <em>若无必要，勿增实体</em> ”；它的源码部分遵循GPLv3，另一部分遵循GPLv2/LGPLv3；统一对象视图，与UNIX设计哲学类似，所有皆对象；跨平台兼容性高，可作为hadoop、openstack、ovirt、Amazon EC的后端。</p>
<img alt="../_images/02-02.png" class="align-center" src="../_images/02-02.png" />
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p><strong>砖块（brick）</strong>：即服务器节点上导出的一个目录，作为glusterfs的最基本单元。</p>
<p><strong>卷（volume）</strong>：用户最终使用的、由砖块组成的逻辑卷。</p>
<p><strong>GFID</strong>：glusterfs中的每一个文件或者目录都有一个独立的128位GFID，与普通文件系统中的inode类似。</p>
<p class="last"><strong>节点（peer）</strong>：即集群中含有砖块并参与构建卷的计算机。</p>
</div>
<div class="section" id="id3">
<h3>功能介绍<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h3>
<p>具体功能特性请参考 <a class="reference external" href="http://gluster.readthedocs.org/en/latest/">Glusterfs features</a> 。</p>
</div>
<div class="section" id="id4">
<h3>组合方式<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p><strong>gluster支持四种存储逻辑卷组合：普通分布式（Distributed）、条带（Striped）、冗余（Replicated）、条带冗余（Striped-Replicated）</strong></p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="26%" />
<col width="74%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>普通分布式</td>
<td><img alt="../_images/02-04.png" class="first last align-center" src="../_images/02-04.png" />
</td>
</tr>
<tr class="row-even"><td>条带</td>
<td><img alt="../_images/02-05.png" class="first last align-center" src="../_images/02-05.png" />
</td>
</tr>
<tr class="row-odd"><td>冗余</td>
<td><img alt="../_images/02-06.png" class="first last align-center" src="../_images/02-06.png" />
</td>
</tr>
<tr class="row-even"><td>条带冗余</td>
<td><img alt="../_images/02-07.png" class="first last align-center" src="../_images/02-07.png" />
</td>
</tr>
</tbody>
</table>
</div></blockquote>
</div>
<div class="section" id="translator">
<h3>Translator<a class="headerlink" href="#translator" title="永久链接至标题">¶</a></h3>
<p>Translator是glusterfs设计时的核心之一，它具有以下功能：</p>
<ul class="simple">
<li>将用户发来的请求转化为对存储的请求，可以是一对一、一对多或者一对零（cache）。</li>
<li>可用修改请求类型、路径、标志，甚至是数据（加密）。</li>
<li>拦截请求（访问控制）。</li>
<li>生成新请求（预取）。</li>
</ul>
<p><strong>类型</strong></p>
<p>根据translator的类型，可用将其分为如下类型：</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="29%" />
<col width="71%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Translator 类型</th>
<th class="head">功能</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Storage</td>
<td>访问本地文件系统。</td>
</tr>
<tr class="row-odd"><td>Debug</td>
<td>提供调试信息。</td>
</tr>
<tr class="row-even"><td>Cluster</td>
<td>处理集群环境下读写请求。</td>
</tr>
<tr class="row-odd"><td>Encryption</td>
<td>加密/解密传送中的数据。</td>
</tr>
<tr class="row-even"><td>Protocol</td>
<td>加密/解密传送中的数据。</td>
</tr>
<tr class="row-odd"><td>Performance</td>
<td>IO参数调节。</td>
</tr>
<tr class="row-even"><td>Bindings</td>
<td>增加可扩展性，比如python接口。</td>
</tr>
<tr class="row-odd"><td>System</td>
<td>负责系统访问，比如文件系统控制接口访问。</td>
</tr>
<tr class="row-even"><td>Scheduler</td>
<td>调度集群环境下文件访问请求。</td>
</tr>
<tr class="row-odd"><td>Features</td>
<td>提供额外文件特性，比如quota，锁机制等。</td>
</tr>
</tbody>
</table>
</div></blockquote>
</div>
<div class="section" id="afr">
<h3>AFR<a class="headerlink" href="#afr" title="永久链接至标题">¶</a></h3>
<p>AFR（Automatic File Replication）是translator的一种，它使用额外机制去控制跟踪文件操作，用于跨砖块复制数据。</p>
<p>支持跨网备份</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="26%" />
<col width="74%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>局域网备份</td>
<td><img alt="../_images/02-08.png" class="first last align-center" src="../_images/02-08.png" />
</td>
</tr>
<tr class="row-even"><td>内网备份</td>
<td><img alt="../_images/02-09.png" class="first last align-center" src="../_images/02-09.png" />
</td>
</tr>
<tr class="row-odd"><td>广域网备份</td>
<td><img alt="../_images/02-10.png" class="first last align-center" src="../_images/02-10.png" />
</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>其中，它有以下特点：</p>
<ul class="simple">
<li>保持数据一致性</li>
<li>发生脑裂时自动恢复，应保证至少一个节点有正确数据</li>
<li>为读系列操作提供最新数据结构</li>
</ul>
</div>
<div class="section" id="dht">
<h3>DHT<a class="headerlink" href="#dht" title="永久链接至标题">¶</a></h3>
<p>DHT（Distributed Hash Table）是glusterfs的真正核心。它决定将每个文件放置至砖块的位置。不同于多副本或者条带模式，它的功能是路由，而不是分割或者拷贝。</p>
<p><strong>工作方式</strong></p>
<p>分布式哈希表的核心是一致性哈希算法，又名环形哈希。它具有的一个性质是当一个存储空间被加入或者删除时，现有得映射关系的改变尽可能小。</p>
<p>假设我们的哈希算出一个32位的哈希值，即一个[0,2^32-1]的空间，现将它首尾相接，即构成一个环形。</p>
<p>假如我们有四个存储砖块，每一个砖块B都有一个哈希值H，假设四个文件及其哈希值表示为(k,v)，那么他们在哈希环上即如此表示：</p>
<img alt="../_images/02-14.png" class="align-center" src="../_images/02-14.png" />
<p>每一个文件哈希k顺时针移动遇到一个H后，就将文件k保存至B。</p>
<img alt="../_images/02-15.png" class="align-center" src="../_images/02-15.png" />
<p>上图表示的是理想环境下文件与砖块的存储映射，当有砖块失效时，存储位置的映射也就发生了改变。比如砖块B3失效，那么文件v3会被继续顺时针改变至B4上。</p>
<img alt="../_images/02-16.png" class="align-center" src="../_images/02-16.png" />
<p>当砖块数目发生改变时，为了服务器能平摊负载，我们需要一次rebalance来稍许改变映射关系。rebalance的技巧即是创建一个虚拟的存储位置B&#8217;，使所有砖块及其虚拟砖块尽量都存储有文件。</p>
<img alt="../_images/02-17.png" class="align-center" src="../_images/02-17.png" />
<img alt="../_images/02-18.png" class="align-center" src="../_images/02-18.png" />
</div>
</div>
<div class="section" id="id5">
<h2>2.3 搭建Glusterfs作为基础存储<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>既然要搭建一个稳健的基础存储，那么glusterfs推荐使用distributed striped replicated方式，这里使用4台预装CentOS 6(SELINUX设置为permissive)的机器进行演示。</p>
<div class="section" id="dnshosts">
<h3>添加DNS或者修改hosts文件<a class="headerlink" href="#dnshosts" title="永久链接至标题">¶</a></h3>
<p>鉴于笔者所在环境中暂时没有配置独立的DNS，此处先修改hosts文件以完成配置，注意每台机器都要添加：</p>
<blockquote>
<div><em>/etc/hosts</em></div></blockquote>
<div class="code highlight-python"><div class="highlight"><pre>127.0.0.1       localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6

192.168.10.101  gs1.example.com
192.168.10.102  tgs2.example.com
192.168.10.103  tgs3.example.com
192.168.10.104  gs4.example.com
</pre></div>
</div>
<p>同样地在所有机器上添加repo：</p>
<blockquote>
<div><em>/etc/yum.repos.d/gluster_epel.repo</em></div></blockquote>
<div class="code highlight-python"><div class="highlight"><pre>[epel]
name=Extra Packages for Enterprise Linux 6 - $basearch
#baseurl=http://download.fedoraproject.org/pub/epel/6/$basearch
mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&amp;amp;arch=$basearch
failovermethod=priority
enabled=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6

[glusterfs-epel]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes.
baseurl=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/epel-$releasever/$basearch/
enabled=1
skip_if_unavailable=1
gpgcheck=0
gpgkey=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/pub.key

[glusterfs-noarch-epel]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes.
baseurl=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/epel-$releasever/noarch
enabled=1
skip_if_unavailable=1
gpgcheck=0
gpgkey=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/pub.key

[glusterfs-source-epel]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes. - Source
baseurl=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/epel-$releasever/SRPMS
enabled=0
skip_if_unavailable=1
gpgcheck=1
gpgkey=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/pub.key
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h3>准备磁盘作为砖块<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<p>在所有节点上安装xfs用户空间工具：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># yum install -y glusterfs glusterfs-fuse glusterfs-server xfsprogs</span>
<span class="c"># /etc/init.d/glusterd start</span>
<span class="c"># /etc/init.d/glusterfsd start</span>
<span class="c"># chkconfig glusterfsd on</span>
<span class="c"># chkconfig glusterd on</span>
</pre></div>
</div>
<p>假设每台机器除系统盘之外都有2块1T SATA硬盘，我们需要对其进行分区，创建逻辑卷，格式化并挂载：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># fdisk /dev/sdX &lt;&lt; EOF</span>
<span class="n">n</span>
<span class="n">p</span>
<span class="mi">1</span>

<span class="n">w</span>
<span class="n">EOF</span>
</pre></div>
</div>
<p>格式化并挂载：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># mkfs.xfs -i size 512 /dev/sdb1</span>
<span class="c"># mkfs.xfs -i size 512 /dev/sdc1</span>
<span class="c"># mkdir /gluster_brick_root1</span>
<span class="c"># mkdir /gluster_brick_root2</span>
<span class="c"># echo -e &quot;/dev/sdb1\t/gluster_brick_root1\txfs\tdefaults\t0 0\n/dev/sdc1\t/gluster_brick_root2\txfs\tdefaults\t0 0&quot; &gt;&gt; /etc/fstab</span>
<span class="c"># mount -a</span>
<span class="c"># mkdir /gluster_brick_root1/data</span>
<span class="c"># mkdir /gluster_brick_root2/data</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p>为什么要用XFS？</p>
<p class="last">XFS具有元数据日志功能，可以快速恢复数据；同时，可以在线扩容及碎片整理。其他文件系统比如EXT3，EXT4未做充分测试。</p>
</div>
</div>
<div class="section" id="id7">
<h3>添加卷<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>在其中任意台机器上，比如gs2.example.com，执行</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># gluster peer probe gs1.example.com</span>
<span class="c"># gluster peer probe gs3.example.com</span>
<span class="c"># gluster peer probe gs4.example.com</span>
</pre></div>
</div>
<p>使用砖块进行卷的构建：</p>
<div class="code highlight-python"><div class="highlight"><pre># gluster
  &gt; volume create gluster-vol1 stripe 2 replica 2 \
  gs1.example.com:/gluster_brick_root1/data gs2.example.com:/gluster_brick_root1/data \
  gs1.example.com:/gluster_brick_root2/data gs2.example.com:/gluster_brick_root2/data \
  gs3.example.com:/gluster_brick_root1/data gs4.example.com:/gluster_brick_root1/data \
  gs3.example.com:/gluster_brick_root2/data gs4.example.com:/gluster_brick_root2/data force
  &gt; volume start gluster-vol1 # 启动卷
  &gt; volume status gluster-vol1 # 查看卷状态
  Status of volume: gluster-vol1
  Gluster process                                         Port    Online  Pid
  ------------------------------------------------------------------------------
  Brick gs1.example.com:/gluster_brick_root1/data         49152   Y       1984
  Brick gs2.example.com:/gluster_brick_root1/data         49152   Y       1972
  Brick gs1.example.com:/gluster_brick_root2/data         49153   Y       1995
  Brick gs2.example.com:/gluster_brick_root2/data         49153   Y       1983
  Brick gs3.example.com:/gluster_brick_root1/data         49152   Y       1961
  Brick gs4.example.com:/gluster_brick_root1/data         49152   Y       1975
  Brick gs3.example.com:/gluster_brick_root2/data         49153   Y       1972
  Brick gs4.example.com:/gluster_brick_root2/data         49153   Y       1986
  NFS Server on localhost                                 2049    Y       1999
  Self-heal Daemon on localhost                           N/A     Y       2006
  NFS Server on gs2.example.com                           2049    Y       2007
  Self-heal Daemon on gs2.example.com                     N/A     Y       2014
  NFS Server on gs2.example.com                           2049    Y       1995
  Self-heal Daemon on gs2.example.com                     N/A     Y       2002
  NFS Server on gs3.example.com                           2049    Y       1986
  Self-heal Daemon on gs3.example.com                     N/A     Y       1993

  Task Status of Volume gluster-vol1
  ------------------------------------------------------------------------------
  There are no active volume tasks
  &gt; volume info all 查看所有卷信息
  gluster volume info all

  Volume Name: gluster-vol1
  Type: Distributed-Striped-Replicate
  Volume ID: bc8e102c-2b35-4748-ab71-7cf96ce083f3
  Status: Started
  Number of Bricks: 2 x 2 x 2 = 8
  Transport-type: tcp
  Bricks:
  Brick1: gs1.example.com:/gluster_brick_root1/data
  Brick2: gs2.example.com:/gluster_brick_root1/data
  Brick3: gs1.example.com:/gluster_brick_root2/data
  Brick4: gs2.example.com:/gluster_brick_root2/data
  Brick5: gs3.example.com:/gluster_brick_root1/data
  Brick6: gs4.example.com:/gluster_brick_root1/data
  Brick7: gs3.example.com:/gluster_brick_root2/data
  Brick8: gs4.example.com:/gluster_brick_root2/data
</pre></div>
</div>
</div>
<div class="section" id="id8">
<h3>挂载卷<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h3>
<p>当以glusterfs挂载时，客户端的hosts文件里需要有的任一节点做解析：</p>
<blockquote>
<div><em>挂载glusterfs的客户端/etc/hosts</em></div></blockquote>
<div class="code highlight-python"><div class="highlight"><pre>127.0.0.1       localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6

192.168.1.81    gs1.example.com
</pre></div>
</div>
<p>安装gluster-fuse，将gluster卷作为glusterfs挂载，并写入1M文件查看其在各砖块分配：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># yum install glusterfs glusterfs-fuse</span>
<span class="c"># mount.glusterfs 192.168.1.81:/gluster-vol1 /mnt</span>
<span class="c"># cd /mnt</span>
<span class="c"># dd if=/dev/zero of=a.img bs=1k count=1k</span>
<span class="c"># cp a.img b.img; cp a.img c.img; cp a.img d.img</span>
</pre></div>
</div>
<p>在四台服务端分别查看：</p>
<div class="code highlight-python"><div class="highlight"><pre>[root@gs1 ~]# ls -lh /gluster_brick_root*
/gluster_brick_root1/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 a.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 d.img
/gluster_brick_root2/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 a.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 d.img
</pre></div>
</div>
<div class="code highlight-python"><div class="highlight"><pre>[root@gs2 ~]# ls -lh /gluster_brick_root*
/gluster_brick_root1/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 a.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 d.img
/gluster_brick_root2/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 a.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 d.img
</pre></div>
</div>
<div class="code highlight-python"><div class="highlight"><pre>[root@gs3 ~]# ls -lh /gluster_brick_root*
/gluster_brick_root1/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 b.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 c.img
/gluster_brick_root2/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 b.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 c.img
</pre></div>
</div>
<div class="code highlight-python"><div class="highlight"><pre>[root@gs4 ~]# ls -lh /gluster_brick_root*
/gluster_brick_root1/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 b.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 c.img
/gluster_brick_root2/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 b.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 c.img
</pre></div>
</div>
<p>至此，所有配置结束。</p>
</div>
</div>
<div class="section" id="id9">
<h2>2.4 Glusterfs应用示例及技巧<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h2>
<div class="section" id="id10">
<h3>参数调整<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="5%" />
<col width="81%" />
<col width="3%" />
<col width="11%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Option</th>
<th class="head">Description</th>
<th class="head">Default Value</th>
<th class="head">Available Options</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>auth.allow</td>
<td>IP addresses of the clients which should be allowed to access the volume.</td>
<td><ul class="first last simple">
<li>(allow all)</li>
</ul>
</td>
<td>Valid IP address which includes wild card patterns including <em>, such as 192.168.1.</em></td>
</tr>
<tr class="row-odd"><td>auth.reject</td>
<td>IP addresses of the clients which should be denied to access the volume.</td>
<td>NONE (reject none)</td>
<td>Valid IP address which includes wild card patterns including <em>, such as 192.168.2.</em></td>
</tr>
<tr class="row-even"><td>client.grace-timeout</td>
<td>Specifies the duration for the lock state to be maintained on the client after a network disconnection.</td>
<td>10</td>
<td>10 - 1800 secs</td>
</tr>
<tr class="row-odd"><td>cluster.self-heal-window-size</td>
<td>Specifies the maximum number of blocks per file on which self-heal would happen simultaneously.</td>
<td>16</td>
<td>0 - 1025 blocks</td>
</tr>
<tr class="row-even"><td>cluster.data-self-heal-algorithm</td>
<td>Specifies the type of self-heal. If you set the option as &#8220;full&#8221;, the entire file is copied from source to destinations. If the option is set to &#8220;diff&#8221; the file blocks that are not in sync are copied to destinations. Reset uses a heuristic model. If the file does not exist on one of the subvolumes, or a zero-byte file exists (created by entry self-heal) the entire content has to be copied anyway, so there is no benefit from using the &#8220;diff&#8221; algorithm. If the file size is about the same as page size, the entire file can be read and written with a few operations, which will be faster than &#8220;diff&#8221; which has to read checksums and then read and write.</td>
<td>reset</td>
<td>full/diff/reset</td>
</tr>
<tr class="row-odd"><td>cluster.min-free-disk</td>
<td>Specifies the percentage of disk space that must be kept free. Might be useful for non-uniform bricks</td>
<td>10%</td>
<td>Percentage of required minimum free disk space</td>
</tr>
<tr class="row-even"><td>cluster.stripe-block-size</td>
<td>Specifies the size of the stripe unit that will be read from or written to.</td>
<td>128 KB (for all files)</td>
<td>size in bytes</td>
</tr>
<tr class="row-odd"><td>cluster.self-heal-daemon</td>
<td>Allows you to turn-off proactive self-heal on replicated</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>cluster.ensure-durability</td>
<td>This option makes sure the data/metadata is durable across abrupt shutdown of the brick.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-odd"><td>diagnostics.brick-log-level</td>
<td>Changes the log-level of the bricks.</td>
<td>INFO</td>
<td>DEBUG/WARNING/ERROR/CRITICAL/NONE/TRACE</td>
</tr>
<tr class="row-even"><td>diagnostics.client-log-level</td>
<td>Changes the log-level of the clients.</td>
<td>INFO</td>
<td>DEBUG/WARNING/ERROR/CRITICAL/NONE/TRACE</td>
</tr>
<tr class="row-odd"><td>diagnostics.latency-measurement</td>
<td>Statistics related to the latency of each operation would be tracked.</td>
<td>Off</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>diagnostics.dump-fd-stats</td>
<td>Statistics related to file-operations would be tracked.</td>
<td>Off</td>
<td>On</td>
</tr>
<tr class="row-odd"><td>features.read-only</td>
<td>Enables you to mount the entire volume as read-only for all the clients (including NFS clients) accessing it.</td>
<td>Off</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>features.lock-heal</td>
<td>Enables self-healing of locks when the network disconnects.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-odd"><td>features.quota-timeout</td>
<td>For performance reasons, quota caches the directory sizes on client. You can set timeout indicating the maximum duration of directory sizes in cache, from the time they are populated, during which they are considered valid</td>
<td>0</td>
<td>0 - 3600 secs</td>
</tr>
<tr class="row-even"><td>geo-replication.indexing</td>
<td>Use this option to automatically sync the changes in the filesystem from Master to Slave.</td>
<td>Off</td>
<td>On/Off</td>
</tr>
<tr class="row-odd"><td>network.frame-timeout</td>
<td>The time frame after which the operation has to be declared as dead, if the server does not respond for a particular operation.</td>
<td>1800 (30 mins)</td>
<td>1800 secs</td>
</tr>
<tr class="row-even"><td>network.ping-timeout</td>
<td>The time duration for which the client waits to check if the server is responsive. When a ping timeout happens, there is a network disconnect between the client and server. All resources held by server on behalf of the client get cleaned up. When a reconnection happens, all resources will need to be re-acquired before the client can resume its operations on the server. Additionally, the locks will be acquired and the lock tables updated. This reconnect is a very expensive operation and should be avoided.</td>
<td>42 Secs</td>
<td>42 Secs</td>
</tr>
<tr class="row-odd"><td>nfs.enable-ino32</td>
<td>For 32-bit nfs clients or applications that do not support 64-bit inode numbers or large files, use this option from the CLI to make Gluster NFS return 32-bit inode numbers instead of 64-bit inode numbers.</td>
<td>Off</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>nfs.volume-access</td>
<td>Set the access type for the specified sub-volume.</td>
<td>read-write</td>
<td>read-write/read-only</td>
</tr>
<tr class="row-odd"><td>nfs.trusted-write</td>
<td>If there is an UNSTABLE write from the client, STABLE flag will be returned to force the client to not send a COMMIT request. In some environments, combined with a replicated GlusterFS setup, this option can improve write performance. This flag allows users to trust Gluster replication logic to sync data to the disks and recover when required. COMMIT requests if received will be handled in a default manner by fsyncing. STABLE writes are still handled in a sync manner.</td>
<td>Off</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>nfs.trusted-sync</td>
<td>All writes and COMMIT requests are treated as async. This implies that no write requests are guaranteed to be on server disks when the write reply is received at the NFS client. Trusted sync includes trusted-write behavior.</td>
<td>Off</td>
<td>On/Off</td>
</tr>
<tr class="row-odd"><td>nfs.export-dir</td>
<td>This option can be used to export specified comma separated subdirectories in the volume. The path must be an absolute path. Along with path allowed list of IPs/hostname can be associated with each subdirectory. If provided connection will allowed only from these IPs. Format: &lt;dir&gt;[(hostspec[hostspec...])][,...]. Where hostspec can be an IP address, hostname or an IP range in CIDR notation. Note: Care must be taken while configuring this option as invalid entries and/or unreachable DNS servers can introduce unwanted delay in all the mount calls.</td>
<td>No sub directory exported.</td>
<td>Absolute path with allowed list of IP/hostname</td>
</tr>
<tr class="row-even"><td>nfs.export-volumes</td>
<td>Enable/Disable exporting entire volumes, instead if used in conjunction with nfs3.export-dir, can allow setting up only subdirectories as exports.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-odd"><td>nfs.rpc-auth-unix</td>
<td>Enable/Disable the AUTH_UNIX authentication type. This option is enabled by default for better interoperability. However, you can disable it if required.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>nfs.rpc-auth-null</td>
<td>Enable/Disable the AUTH_NULL authentication type. It is not recommended to change the default value for this option.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-odd"><td>nfs.rpc-auth-allow&lt;IP- Addresses&gt;</td>
<td>Allow a comma separated list of addresses and/or hostnames to connect to the server. By default, all clients are disallowed. This allows you to define a general rule for all exported volumes.</td>
<td>Reject All</td>
<td>IP address or Host name</td>
</tr>
<tr class="row-even"><td>nfs.rpc-auth-reject&lt;IP- Addresses&gt;</td>
<td>Reject a comma separated list of addresses and/or hostnames from connecting to the server. By default, all connections are disallowed. This allows you to define a general rule for all exported volumes.</td>
<td>Reject All</td>
<td>IP address or Host name</td>
</tr>
<tr class="row-odd"><td>nfs.ports-insecure</td>
<td>Allow client connections from unprivileged ports. By default only privileged ports are allowed. This is a global setting in case insecure ports are to be enabled for all exports using a single option.</td>
<td>Off</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>nfs.addr-namelookup</td>
<td>Turn-off name lookup for incoming client connections using this option. In some setups, the name server can take too long to reply to DNS queries resulting in timeouts of mount requests. Use this option to turn off name lookups during address authentication. Note, turning this off will prevent you from using hostnames in rpc-auth.addr.* filters.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-odd"><td>nfs.register-with-portmap</td>
<td>For systems that need to run multiple NFS servers, you need to prevent more than one from registering with portmap service. Use this option to turn off portmap registration for Gluster NFS.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>nfs.port &lt;PORT- NUMBER&gt;</td>
<td>Use this option on systems that need Gluster NFS to be associated with a non-default port number.</td>
<td>NA</td>
<td>38465- 38467</td>
</tr>
<tr class="row-odd"><td>nfs.disable</td>
<td>Turn-off volume being exported by NFS</td>
<td>Off</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>performance.write-behind-window-size</td>
<td>Size of the per-file write-behind buffer.</td>
<td>1MB</td>
<td>Write-behind cache size</td>
</tr>
<tr class="row-odd"><td>performance.io-thread-count</td>
<td>The number of threads in IO threads translator.</td>
<td>16</td>
<td>0-65</td>
</tr>
<tr class="row-even"><td>performance.flush-behind</td>
<td>If this option is set ON, instructs write-behind translator to perform flush in background, by returning success (or any errors, if any of previous writes were failed) to application even before flush is sent to backend filesystem.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-odd"><td>performance.cache-max-file-size</td>
<td>Sets the maximum file size cached by the io-cache translator. Can use the normal size descriptors of KB, MB, GB,TB or PB (for example, 6GB). Maximum size uint64.</td>
<td>2 ^ 64 -1 bytes</td>
<td>size in bytes</td>
</tr>
<tr class="row-even"><td>performance.cache-min-file-size</td>
<td>Sets the minimum file size cached by the io-cache translator. Values same as &#8220;max&#8221; above</td>
<td>0B</td>
<td>size in bytes</td>
</tr>
<tr class="row-odd"><td>performance.cache-refresh-timeout</td>
<td>The cached data for a file will be retained till &#8216;cache-refresh-timeout&#8217; seconds, after which data re-validation is performed.</td>
<td>1s</td>
<td>0-61</td>
</tr>
<tr class="row-even"><td>performance.cache-size</td>
<td>Size of the read cache.</td>
<td>32 MB</td>
<td>size in bytes</td>
</tr>
<tr class="row-odd"><td>server.allow-insecure</td>
<td>Allow client connections from unprivileged ports. By default only privileged ports are allowed. This is a global setting in case insecure ports are to be enabled for all exports using a single option.</td>
<td>On</td>
<td>On/Off</td>
</tr>
<tr class="row-even"><td>server.grace-timeout</td>
<td>Specifies the duration for the lock state to be maintained on the server after a network disconnection.</td>
<td>10</td>
<td>10 - 1800 secs</td>
</tr>
<tr class="row-odd"><td>server.statedump-path</td>
<td>Location of the state dump file.</td>
<td>tmp directory of the brick</td>
<td>New directory path</td>
</tr>
<tr class="row-even"><td>storage.health-check-interval</td>
<td>Number of seconds between health-checks done on the filesystem that is used for the brick(s). Defaults to 30 seconds, set to 0 to disable.</td>
<td>tmp directory of the brick</td>
<td>New directory path</td>
</tr>
</tbody>
</table>
<p>具体参数参考 <a class="reference external" href="http://gluster.readthedocs.org/en/release-3.7.0-1/Administrator%20Guide/Managing%20Volumes/">gluster_doc</a> 。</p>
</div>
<div class="section" id="id11">
<h3>文件权限<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h3>
<p>glusterfs在创建卷时会更改砖块所有者为root.root，对于某些应用请注意更改砖块目录所有者（比如在/etc/rc.local中添加chown，不要更改砖块下隐藏目录.glusterfs）。</p>
</div>
<div class="section" id="id12">
<h3>砖块组合<a class="headerlink" href="#id12" title="永久链接至标题">¶</a></h3>
<p>网上现有的部分文档中所述的砖块划分方式，是将整个磁盘划分为砖块，此种划分方式在某些场景下不是很好（比如存储复用），可以在/brickX下创建目录，比如data1，同时在创建glusterfs卷的时候使用HOST:/brickX/data1作为砖块，以合理利用存储空间。</p>
</div>
<div class="section" id="normalreplicastriped">
<h3>normal、replica、striped卷组合<a class="headerlink" href="#normalreplicastriped" title="永久链接至标题">¶</a></h3>
<p>砖块的划分排序：striped（normal）优先，replica在striped（normal）基础上做冗余；计算大小时，同一replica组中的brick合并为一个砖块，一个striped组可看做一个有效块。</p>
<p>假设我们有4个主机，8个砖块，每个砖块都是5GB，如下图：</p>
<blockquote>
<div><img alt="../_images/02-11.png" class="align-center" src="../_images/02-11.png" />
</div></blockquote>
<p>创建卷时使用如下命令：</p>
<div class="code highlight-python"><div class="highlight"><pre># gluster volume create gluster-vol1 stripe 2 replica 2 \
host1:/brick1 host1:/brick2 host2:/brick1 host2:/brick2 \
host3:/brick1 host3:/brick2 host4:/brick1 host4:/brick2 force
</pre></div>
</div>
<p>砖块将会按照如下进行组合：</p>
<blockquote>
<div><img alt="../_images/02-12.png" class="align-center" src="../_images/02-12.png" />
</div></blockquote>
<p>然而，创建卷时使用如下命令：</p>
<div class="code highlight-python"><div class="highlight"><pre># gluster volume create gluster-vol1 stripe 2 replica 2 \
host1:/brick1 host2:/brick1 host3:/brick1 host4:/brick1 \
host1:/brick2 host2:/brick2 host3:/brick2 host4:/brick2 force
</pre></div>
</div>
<p>砖块将会按照如下进行组合：</p>
<blockquote>
<div><img alt="../_images/02-13.png" class="align-center" src="../_images/02-13.png" />
</div></blockquote>
</div>
<div class="section" id="nfs">
<h3>作为nfs挂载<a class="headerlink" href="#nfs" title="永久链接至标题">¶</a></h3>
<p>由于glusterfs占用了2049端口，所以其与nfs server一般不能共存于同一台服务器，除非更改nfs服务端口。</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># mount -t nfs -o vers=3 server1:/volume1 /mnt</span>
</pre></div>
</div>
</div>
<div class="section" id="cifs">
<h3>作为cifs挂载<a class="headerlink" href="#cifs" title="永久链接至标题">¶</a></h3>
<p>先在某一服务器或者客户端将起挂载，再以cifs方式导出：</p>
<blockquote>
<div>/etc/smb.conf</div></blockquote>
<div class="code highlight-python"><div class="highlight"><pre>[glustertest]
comment = For testing a Gluster volume exported through CIFS
path = /mnt/glusterfs
read only = no
guest ok = yes
</pre></div>
</div>
</div>
<div class="section" id="split-brain">
<h3>修复裂脑（split-brain）<a class="headerlink" href="#split-brain" title="永久链接至标题">¶</a></h3>
<p>裂脑发生以后，各节点信息可能会出现不一致。可以通过以下步骤查看并修复。</p>
<ol class="arabic simple">
<li>定位裂脑文件</li>
</ol>
<p>通过命令</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># gluster volume heal info split-brain</span>
</pre></div>
</div>
<p>或者查看在客户端仍然是Input/Output错误的文件。</p>
<ol class="arabic simple" start="2">
<li>关闭已经打开的文件或者虚机</li>
<li>确定正确副本</li>
<li>恢复扩展属性</li>
</ol>
<p>登录到后台，查看脑裂文件的MD5sum和时间，判断哪个副本是需要保留的。
然后删除不再需要的副本即可。（glusterfs采用硬链接方式，所以需要同时删除.gluster下面的硬连接文件）</p>
<p>首先检查文件的md5值，并且和其他的节点比较，确认是否需要删除此副本。</p>
<div class="code highlight-python"><div class="highlight"><pre>[root@hostd data0]# md5sum 1443f429-7076-4792-9cb7-06b1ee38d828/images/5c881816-6cdc-4d8a-a8c8-4b068a917c2f/80f33212-7adb-4e24-9f01-336898ae1a2c
6c6b704ce1c0f6d22204449c085882e2 1443f429-7076-4792-9cb7-06b1ee38d828/images/5c881816-6cdc-4d8a-a8c8-4b068a917c2f/80f33212-7adb-4e24-9f01-336898ae1a2c
</pre></div>
</div>
<p>通过ls -i 和find -inum 找到此文件及其硬连接文件。</p>
<p>删除两个文件</p>
<div class="code highlight-python"><div class="highlight"><pre>[root@hostd data0]# find -inum 12976365 |xargs rm -rf
</pre></div>
</div>
<p>脑裂文件恢复完成，此文件可以在挂载点上读写。</p>
</div>
<div class="section" id="id13">
<h3>砖块复用<a class="headerlink" href="#id13" title="永久链接至标题">¶</a></h3>
<p>当卷正在被使用，其中一个砖块被删除，而用户试图再次将其用于卷时，可能会出现“/bricks/app or a prefix of it is already part of a volume”。</p>
<p>解决方法：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># setfattr -x trusted.glusterfs.volume-id $brick_path</span>
<span class="c"># setfattr -x trusted.gfid $brick_path</span>
<span class="c"># rm -rf $brick_path/.glusterfs</span>
</pre></div>
</div>
</div>
<div class="section" id="ip">
<h3>高可用业务IP<a class="headerlink" href="#ip" title="永久链接至标题">¶</a></h3>
<p>由于挂载存储时需要指定集群中的任意IP，所以我们可以使用Heartbeat/CTDB/Pacemaker等集群软件来保证业务IP的高可用。</p>
<p>可参考</p>
<p><a class="reference external" href="http://clusterlabs.org/wiki/Debian_Lenny_HowTo#Configure_an_IP_resource">http://clusterlabs.org/wiki/Debian_Lenny_HowTo#Configure_an_IP_resource</a></p>
<p><a class="reference external" href="http://geekpeek.net/linux-cluster-corosync-pacemaker/">http://geekpeek.net/linux-cluster-corosync-pacemaker/</a></p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ch03.html" class="btn btn-neutral float-right" title="第三章 合适的虚拟化平台" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ch01.html" class="btn btn-neutral" title="第一章 随便说些什么" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; 版权所有 2014, lofyer.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/translations.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>